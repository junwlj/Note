# 回归算法

**回归的目的是预测数值型的目标值**

### 回归准确性评价：

​		对于已经建立好的回归器，最重要的就是评价回归器的拟合效果。模型评价通过使用误差（error）来评价拟合效果，即实际值与模型预测值之间的差值。

​	误差$\varepsilon^{(i)}$是独立并且具有相同的分布，并且服从均值为0的方差为$\theta^2$的高斯分布

- 平均绝对误差（mean absolute error）：给定数据集的所有数据点的绝对误差平均值
- 均方误差（mean squared error）：给定数据集的所有数据点误差的平方的平均值
- 中位数绝对误差（median absolute error）：给定数据集的所有数据点的误差的中位数。这个指标的主要优点是可以消除异常值（outlier）的干扰。测试数据集中的单个坏点不会影响整个误差指标，均值误差指标会受到异常点的影响
- 解释均方误差（explained variance score）：这个分数是用于衡量我们的模型对数据集波动的解释能力。如果得分为1.0分，则表示我们的模型是完美的。
- R方得分（R2 score）：该指标是通过指定相关系数，用于衡量模型对未知样本的预测的效果，最好的得分是1.0，值也可为负数。
	- $1- \frac{\sum_{i=1}^{m}(\hat{y_i}-y_i)^2}{\sum_{i=1}^{m}(y_i-\bar{y})^2}$
	- $\sum_{i=1}^{m}(\hat{y_i}-y_i)^2$  残差平方和
	- $\sum_{i=1}^{m}(y_i-\bar{y})^2$ 类似方差项

### 线性回归

- 解决线性连续值预测的问题
- 使用最小二乘法计算误差平方和，再通过对w求导计算最小误差系数

拟合的平面：$h_0(x) = \theta_0+\theta_1x_1 + \theta_2x_2$ （$\theta_0$是偏置顶）

整合：$h_0(x) = \sum^{n}_{i=0}\theta_ix_i = \theta^Tx$



#### 最小二乘法：

 **公式** $H = \sum_{0}^{m}(y - y_i)^2$

​		最小二乘法是一种学习优化函数，通过最小化误差的平方和寻找数据的最佳函数匹配。

​		使用最小二乘法可以简便的求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。

​		最小二乘法还可以使用于曲线的最佳拟合。

#### 线性回归的特点：

- 优点：结果易于理解，计算不复杂
- 缺点：对线性的数据拟合效果不好
- 适用数据类型：数值型和标称型数据

#### 推导过程 ：

- 预测值和误差：$y^{(i)} = \theta^Tx^{(i)}+\varepsilon^{(i)}$      (1)

- 高斯分布：$p(\varepsilon^{(i)}) = \frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{(\varepsilon^{(i)})^2}{2\sigma^2})$	(2)

- 将（1）代入（2）得：

	- $p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\theta^2})$

- 似然函数：$L(\theta) = \Pi^{m}_{i=1}p(y^{(i)}|x^{(i)};\theta) =  \Pi^{m}_{i=1}\frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\theta^2})$

	- 似然函数是根据样本估计参数值
	- 最大似然估计：即将预测值和样本

- 对数似然：$log(L(\theta)) = log( \Pi^{m}_{i=1}\frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\theta^2}))$

	​									$= mlog\frac{1}{\sqrt{2\pi\sigma}} - \frac{1}{\sigma^2}*\frac{1}{2}\sum^{m}_{i=1}(y^{(i)}-\theta^Tx^{(i)})^2$

	​					

- 最小二乘：$J(\theta) = \frac{1}{2}\sum^{m}_{i=1}(y^{(i)}-\theta^Tx^{(i)})^2 = \frac{1}{2}(X\theta - y)^T(X\theta -y)$

- 求偏导：$\nabla_{\theta}J(\theta) = \nabla_{\theta}(\frac{1}{2}(X\theta-y)^2(X\theta-y))=\nabla_{\theta}(\frac{1}{2}(\theta^TX^T-Y^T)(X\theta-y))$

	​								$=\nabla_{\theta}(\frac{1}{2}(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^Ty))$

	​								$ =\frac{1}{2}(2X^TX\theta-X^Ty-(y^TX)^T)=X^TX\theta-X^Ty$ 

- 当偏导为0：$\theta = (X^TX)^{-1}X^Ty$

#### 算法实现

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split as split

# 使用scikit—learn中的数据集
# 加载数据集
diabetes = load_diabetes()
# 使用train_test_split切分数据集
train_X, train_Y, test_X, test_Y = split(diabetes.data, diabetes.target, test_size=0.10)
# 加载线性回归算法
linear = LinearRegression()
# 定义特征和标签
samples = diabetes.data[:, 0]
target = diabetes.target
# 使用linear训练数据
linear.fit(samples.reshape(-1, 1), target)
# 测试数据
test_X = np.linspace(samples.min(), samples.max(), num=50)
# 喂入数据
y_ = linear.predict(test_X.reshape(-1,1))
plt.scatter(samples, target)
plt.plot(test_X, y_, color='red')
plt.show()
```



### 岭回归（Ridge）

​		在使用数据的时候通常会出现这么一种情况，即数据的特征要大于样本的数量，此时，将该数据集转换为矩阵时，会发现，矩阵并不是满秩矩阵，因此为了解决不满秩矩阵，我们则使用岭回归（ridge regression）

#### 概念：

​		岭回归即我们所说的L2正则曲线，即在一般的线性回归当中增加了一个参数为w的L2范数的惩罚项，从而最小化残差平方和：

​												$min\rVert{Xw -y}\rVert_{2}^{2}+\lambda\rVert{w}\Vert_{2}^2$

即岭回归就是在普通线性回归函数的基础上增加单位矩阵：

​												$\hat{\omega} = (X^TX + \lambda{I})^{-1}X^Ty$

#### 优点：

- 通过缩减方法可以去掉不重要的参数，因此能够更好地理解数据，通过缩减系数，能够更好地预测结果
- 岭回归是加了二阶正则项的最小二乘，主要适用于过拟合严重或者各变量之间存在多种共线性的时候，岭回归是有bias（偏离率），偏离率是为让variance（差异）更小

1. 岭回归可以解决特征数量比样本量多的问题
2. 岭回归作为一种缩减算法可以判断那些特征重要或者不重要，有点类似降维的效果
3. 缩减算法可以看做是一个模型增加偏差的同时减少方差

#### 应用场景：

1. 数据点少于变量个数
2. 变量之间存在共线性（最小二乘回归得到的系数不稳定，方差很大）
	- 多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或者高度相关关系而使模型估计失真或难以估计准确
3. 应用场景就是处理高度相关的数据

#### Python实现：



### Loss回归

​		最小绝对值收缩和选择算法

​		

### 岭回归



